# NeurIPS Large Language Model Efficiency Challenge Submission

This is my evaluation submission for the NeurIPS LLM efficiency challenge. This is a **student-only** team.

To start the server for the `neurips/local` model, build and run the Dockerfile on any device with a NVIDIA GPU. The model was trained and tested on an RTX 4090 with 24GB of memory. The server will start on port 80 by default.