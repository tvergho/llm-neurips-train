# NeurIPS Large Language Model Efficiency Challenge Submission

Work in progress! This is my evaluation submission for the NeurIPS LLM efficiency challenge.

To start the server for the `neurips/local` model, build and run the Dockerfile on any device with a NVIDIA GPU. The model was trained and tested on an RTX 4090 with 24GB of memory. The server will start on port 80 by default.